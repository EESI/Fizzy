#!/usr/bin/env python
import os
import re
import sys
import csv
import argparse
import json
import numpy
import threading
import itertools
from sklearn.linear_model import LassoCV

__authors__ = [ "Gregory Ditzler", "Calvin Morrison" ]
__copyright__ = "Copyright 2014, EESI Laboratory (Drexel University)"
__license__ = "GPL"
__maintainer__ = "Gregory Ditzler"
__email__ = "gregory.ditzler@gmail.com"
__status__ = "v1.1.0"

def load_biom_with_meta(args):
  """
  load a biom file and return a dense matrix
  :args - arguments from the parser
  :data - numpy array containing the OTU matrix
  :samples - list containing the sample IDs (important for knowing
    the labels in the data matrix)
  :features - list containing the feature names
  """
  fname = args.input_file

  try:
    o = json.loads(open(fname,"U").read())
  except Exception, e:
    exit('Error: could not load biom. "' + str(e) +'"')

  if o["matrix_type"] == "sparse":
    data = load_sparse(o)
  else:
    data = load_dense(o)

  feature_ids = []
  samples = []
  labels  = []
  for sid in o["columns"]:
    samples.append(sid["id"])
    labels.append(sid["metadata"][args.label])

  features = []
  for sid in o["rows"]:
    # check to see if the taxonomy is listed, this will generally lead to more 
    # descriptive names for the taxonomies. 
    if sid.has_key("metadata") and sid["metadata"] != None:
      if sid["metadata"].has_key("taxonomy"):
        # using json.dumps is a quick way to convert the dictionary to a string
        features.append(json.dumps(sid["metadata"]["taxonomy"]))
        features.append(json.dumps(sid["metadata"]["taxonomy"]))
      else:
        features.append(sid["id"])
    else:
      features.append(sid["id"])
    feature_ids.append(sid["id"])

  return data, samples, features, feature_ids, labels


def load_biom(args):
  """
  load a biom file and return a dense matrix
  :args - arguments from the parser
  :data - numpy array containing the OTU matrix
  :samples - list containing the sample IDs (important for knowing
    the labels in the data matrix)
  :features - list containing the feature names
  """
  fname = args.input_file

  try:
    o = json.loads(open(fname,"U").read())
  except Exception, e:
    exit('Error: could not load biom. "' + str(e) +'"')

  if o["matrix_type"] == "sparse":
    data = load_sparse(o)
  else:
    data = load_dense(o)

  #data = count2abun(1.0+data)
  #data = numpy.ceil(data/numpy.min(data)) # MAYBE I CAN REMOVE THIS

  feature_ids = []
  samples = []
  for sid in o["columns"]:
    samples.append(sid["id"])
  features = []
  for sid in o["rows"]:
    # check to see if the taxonomy is listed, this will generally lead to more 
    # descriptive names for the taxonomies. 
    if sid.has_key("metadata") and sid["metadata"] != None:
      if sid["metadata"].has_key("taxonomy"):
        # using json.dumps is a quick way to convert the dictionary to a string
        features.append(json.dumps(sid["metadata"]["taxonomy"]))
        features.append(json.dumps(sid["metadata"]["taxonomy"]))
      else:
        features.append(sid["id"])
    else:
      features.append(sid["id"])
    feature_ids.append(sid["id"])

  return data, samples, features, feature_ids


def load_svf(args, sep=","):
  """
  load a separated variables file
  """
  data_list = []
  samples = []
  features = []
  feature_ids = []

  with open(args.input_file, "rb") as csvfile:
    reader = csv.reader(csvfile, delimiter=sep)
    for n, row in enumerate(reader):
      if n == 0:
        samples = row[1:]
      else:
        data_list.append([float(x) for x in row[1:]])
        features.append(row[0])
        feature_ids.append("Feature"+str(n))
  data = numpy.array(data_list).transpose()
  return data, samples, features, feature_ids

def load_dense(obj):
  """
  load a biom file in dense format
  :obj - json dictionary from biom file
  :data - dense data matrix
  """
  n_feat,n_sample = obj["shape"]
  data = numpy.array(obj["data"], order="F")
  return data.transpose()

def load_sparse(obj):
  """
  load a biom file in sparse format
  :obj - json dictionary from biom file
  :data - dense data matrix
  """
  n_feat,n_sample = obj["shape"] 
  data = numpy.zeros((n_feat, n_sample),order="F")
  for val in obj["data"]:
    data[val[0], val[1]] = val[2]
  data = data.transpose() 
  return data

def load_map(fn):
  """
  load a map file. this function does not have any dependecies on qiime"s
  tools. the returned object is a dictionary of dictionaries. the dictionary
  is indexed by the sample_ID and there is an added field for the the
  available meta-data. each element in the dictionary is a dictionary with
  the keys of the meta-data.
  :fn - string containing the map file path
  :meta_data - dictionary containin the mapping file information
  """
  meta_data = {}

  with open(fn, "rb") as fh:
   
    first = fh.readline()

    if first[0] != "#":
      exit('Error: expected tab delimted field labels starting with # in map file on line 1')

    first = first.strip('#')

    reader = csv.DictReader(itertools.chain([first], fh), delimiter="\t")

    try:
      reader_arr = [row for row in reader]
      headers = reader.fieldnames
    except csv.Error as e:
      exit("Error: map file contains error at line %d: %s" % (reader.line_num, e))

    if "SampleID" not in headers:
      exit("Error: no SampleID column in map file")

    labels = filter(lambda label: label != "SampleID", headers)

    for row in reader_arr:
      meta_data[row["SampleID"]] = {}
      for label in labels:
        meta_data[row["SampleID"]][label] = row[label]

    return meta_data

def convert_to_discrete(items):
  map_dic = {}
  discrete_arr = []

  # skip the "sample"
  disc_val = 0
  for item in items:
    if item not in map_dic:
       map_dic[item] = disc_val
       disc_val += 1
    discrete_arr.append(map_dic[item])

  return (map_dic, discrete_arr)

def load_config(args):
  """
  load_parameter_file(params_fp)
    @params_fp

    Load a parameters file describing a base classifier.

    In general you have: 
    <parameter name>:<value>:<type>

    Available types are:
    - None
    - float
    - int
    - bool
    - string 
  """
  params = {}
  params["n_alphas"] = 25
  params["fit_intercept"] = True
  params["normalize"] =  True
  params["precompute"] = "auto"
  params["copy_X"] = True
  params["max_iter"] = 1000
  params["tol"] =0.0001
  params["config"] = 0.000001
  params["n_jobs"] = -1

  if args.config != None:
    handl = open(args.config, "U")
    
    for line in handl:
                                                                 
      try:
        line_split = line[:-1].split(":")
      except:
        continue
    
      if len(line_split) != 3: 
        continue
      
      if line_split[2].strip() == "None":
        params[line_split[0]] = None
      elif line_split[2].strip() == "float":
        params[line_split[0]] = float(line_split[1])
      elif line_split[2].strip() == "int":
        params[line_split[0]] = int(line_split[1])
      elif line_split[2].strip() == "bool":
        params[line_split[0]] = bool(line_split[1])
      elif line_split[2].strip() == "string":
        params[line_split[0]] = line_split[1]
    
  return params

def run_lasso(data, samples, features, feature_ids, args, labels, labels_map, config):
  """
  run lasso
  """
  mdl = LassoCV(n_alphas=config["n_alphas"], 
              fit_intercept=config["fit_intercept"], 
              normalize=config["normalize"], 
              precompute=config["precompute"], 
              copy_X=config["copy_X"], 
              max_iter=config["max_iter"],
              n_jobs=config["n_jobs"],
              tol=config["tol"], 
              alphas=None,
              cv=5)
  mdl.fit(X=data, y=labels)
  coeffs = mdl.coef_
  sf = numpy.where(mdl.coef_ > 0.)[0]
  
  if len(sf) == 0:
    raise ValueError("Zero features selected. Possibly due to poor parameter selection.")
  
  reduced_set = []
  reduced_ids = []

  dels = re.compile('"| |\[|\]')
  for k in range(len(sf)):
    mstr = features[int(sf[k])]
    reduced_set.append(dels.sub("", mstr))
    reduced_ids.append(feature_ids[int(sf[k])])

  ds = get_stats(data, labels, sf, reduced_set, labels_map)
  ds["Feature IDs"] = reduced_ids

  if args.json:
    json.dump(ds, open(args.output_file,"w"))
  else:
    f = open(args.output_file, "w")
    f.write("Features\tFeature IDs\t")
    f.write("\t".join(filter(lambda x: x != "Features" and x != "Feature IDs",
      ds.keys()))+"\n")
    
    for n in range(len(ds["Features"])):
      f.write(ds["Features"][n]+"\t"+ds["Feature IDs"][n]+"\t")
      f.write("\t".join([str(ds[x][n]) for x in filter(lambda x: x != "Features" 
        and x != "Feature IDs",ds.keys())])+"\n")
    f.close()
  if args.output_biom is not None:
    write_reduced_biom(data, samples, features, feature_ids, sf, args.output_biom)
  return None


def get_stats(data, labels, indices, reduced_set, key_map):
  """
  Convert a few statistics about the data to a dictionary. 
  """
  data_struct = {}
  data_rel = count2abun(data)
  feature_idx = numpy.array([int(i) for i in indices]).reshape(len(indices),1)
  for n,key in enumerate(key_map):
    class_idx = numpy.where(numpy.array(labels)==key_map[key])[0]
    data_struct[key+" (mean)"] = data_rel[class_idx, feature_idx].mean(axis=1).tolist()
    data_struct[key+" (std)"] = data_rel[class_idx, feature_idx].std(axis=1).tolist()
  data_struct["Features"] = reduced_set
  return data_struct

def count2abun(count_matrix):
  """
  Convert X into a relative abundance matrix
  """
  scale_factor = count_matrix.sum(axis=1)
  return count_matrix/numpy.tile(scale_factor,[count_matrix.shape[1],1]).transpose()

def write_reduced_biom(data, samples, features, feature_ids, sf, output_fn):
  """
  writes relative abundance of top selected features into new biom file.
  """
  import datetime

  biom = {}
  biom['id'] = None
  biom['format'] = u'Biological Observation Matrix 1.0.0'
  biom['data'] = []
  biom['rows'] = []
  biom['columns'] = []
  biom['matrix_element_type'] = u'float'
  biom['matrix_type'] = u'dense'
  biom['type'] = u'OTU table'
  biom['date'] = str(datetime.datetime.now())

  data = count2abun(data)

  reduced_data = []

  for row in data:
    temp_row = []
    for index in sf:
      index = int(index)
      temp_row.append(row[index])
    reduced_data.append(temp_row)

  for index in range(len(data)):
    cols_dic = {}
    cols_dic[u'id'] = samples[index]
    cols_dic[u'metadata'] = {}
    biom['columns'].append(cols_dic)

  for index in sf:
    index = int(index)
    # feature row
    rows_dic = {} 
    rows_dic[u'id'] = feature_ids[index]
    rows_dic[u'metadata'] = { u'taxonomy': features[index] }
    biom['rows'].append(rows_dic)

  # data
  biom['data'] = reduced_data
  json.dump(biom, open(output_fn, "w"))

def build_parser():
  parser = argparse.ArgumentParser(
    description=("Fizzy implements feature subset selection for biological "
      "data formats, which are commonly used in metagenomic data analysis.\n")
  )
  parser.add_argument("-l", "--label",
    help="name of column of the mapping file that "
      +"indicates the labels",
    required=True)
  parser.add_argument("-i", "--input-file",
    help="biom format file",
    required=True)
  parser.add_argument("-m", "--map-file", 
    help="map file (tsv)",
    required=False,
    default=None)
  parser.add_argument("-j", "--json",
    help="store the output as a json format file.",
    action="store_true",
    required=False)
  parser.add_argument("-o", "--output-file",
    help="output file where selected OTU IDs and averages of the OTUS for each class."
    " are stored",
    required=True)
  parser.add_argument("-r", "--output-biom",
    help="output a BIOM file with the relative abundances of the sub-matrix of"
    " the selected features.",
    required=False)
  parser.add_argument("-v", "--svf",
    help="the input file is a separated variables file",
    action="store_true",
    required=False)
  parser.add_argument("-c", "--config",
    help="file path to lasso config file. see sklearn's documentation on the "
    "options available to the user.",
    required=False)
  return parser

def main():
  parser = build_parser()
  args = parser.parse_args()

  # Make sure our input exist
  if not os.path.isfile(args.input_file):
    parser.error("Input file not found.")

  if args.map_file == None:
    data, samples, features, feature_ids, labels = load_biom_with_meta(args)
  else:
    if not os.path.isfile(args.map_file):
      parser.error("Map file not found.")
    
    if args.svf:
      data, samples, features, feature_ids = load_svf(args)
    else:
      data, samples, features, feature_ids = load_biom(args)
  
    map_arr = load_map(args.map_file)
    labels = []
    for sample_id in samples:
      if sample_id not in map_arr:
        exit('Error: sample ' + sample_id + ' was not found in map file.')
      if args.label not in map_arr[sample_id]:
        exit('Error: label ' + args.label + ' was not found in map file.')
      labels.append(map_arr[sample_id][args.label])

  labels_disc_dic, labels_disc_arr = convert_to_discrete(labels)

  if len(numpy.unique(labels_disc_arr)) != 2:
    raise ValueError("Unique classes must be equal to 2.")
  
  for n in range(len(labels_disc_arr)):
    if labels_disc_arr[n] == 0:
      labels_disc_arr[n] = -1.
    else: 
      labels_disc_arr[n] = 1.

  config= load_config(args)
  t = threading.Thread(target=run_lasso,
      args=[data, samples, features, feature_ids, args, labels_disc_arr, labels_disc_dic, config])
  t.daemon = True
  t.start()
  while t.is_alive(): # wait for the thread to exit
    t.join(1)

if __name__ == "__main__":
  sys.exit(main())


