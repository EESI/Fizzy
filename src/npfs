#!/usr/bin/env python
import os
import re
import sys
import argparse
import numpy as np
import json
import numpy
import threading
import csv
import itertools
import multiprocessing

__authors__ = [ "Gregory Ditzler", "Calvin Morrison" ]
__copyright__ = "Copyright 2014, EESI Laboratory (Drexel University)"
__license__ = "GPL"
__maintainer__ = "Gregory Ditzler"
__email__ = "gregory.ditzler@gmail.com"
__status__ = "v1.0.0"

def load_biom(args):
  """
  load a biom file and return a dense matrix
  :fname - string containing the path to the biom file
  :data - numpy array containing the OTU matrix
  :samples - list containing the sample IDs (important for knowing
    the labels in the data matrix)
  :features - list containing the feature names
  """
  fname = args.input_file
  try:
    o = json.loads(open(fname,"U").read())
  except Exception, e:
    exit('Error: could not load biom. "' + str(e) +'"')

  if o["matrix_type"] == "sparse":
    data = load_sparse(o)
  else:
    data = load_dense(o)
  
  if args.scale == True or args.log_scale == True:
    data = count2abun(1.0+data)
    if not args.log_scale:
      data = numpy.ceil(data/numpy.min(data))
    else:
      data = numpy.ceil(numpy.log(data/numpy.min(data)))

  feature_ids = []
  samples = []
  for sid in o["columns"]:
    samples.append(sid["id"])
  features = []
  for sid in o["rows"]:
    # check to see if the taxonomy is listed, this will generally lead to more 
    # descriptive names for the taxonomies. 
    if sid.has_key("metadata") and sid["metadata"] != None:
      if sid["metadata"].has_key("taxonomy"):
        # using json.dumps is a quick way to convert the dictionary to a string
        features.append(json.dumps(sid["metadata"]["taxonomy"]))
      else:
        features.append(sid["id"])
    else:
      features.append(sid["id"])
    feature_ids.append(sid["id"])
  return data, samples, features, feature_ids

def load_dense(obj):
  """
  load a biom file in dense format
  :obj - json dictionary from biom file
  :data - dense data matrix
  """
  n_feat,n_sample = obj["shape"]
  data = numpy.array(obj["data"], order="F")
  return data.transpose()

def load_sparse(obj):
  """
  load a biom file in sparse format
  :obj - json dictionary from biom file
  :data - dense data matrix
  """
  n_feat,n_sample = obj["shape"]
  data = numpy.zeros((n_feat, n_sample),order="F")
  for val in obj["data"]:
    data[val[0], val[1]] = val[2]
  data = data.transpose()
  return data

def load_map(fn):
  """
  load a map file. this function does not have any dependecies on qiime"s
  tools. the returned object is a dictionary of dictionaries. the dictionary
  is indexed by the sample_ID and there is an added field for the the
  available meta-data. each element in the dictionary is a dictionary with
  the keys of the meta-data.
  :fn - string containing the map file path
  :meta_data - dictionary containin the mapping file information
  """
  meta_data = {}

  with open(fn, "rb") as fh:
   
    first = fh.readline()

    if first[0] != "#":
      exit('Error: expected tab delimted field labels starting with # in map file on line 1')

    first = first.strip('#')

    reader = csv.DictReader(itertools.chain([first], fh), delimiter='\t')

    try:
      reader_arr = [row for row in reader]
      headers = reader.fieldnames
    except csv.Error as e:
      exit('Error: map file contains error at line %d: %s' % (reader.line_num, e))

    if 'SampleID' not in headers:
      exit('Error: no SampleID column in map file')

    labels = filter(lambda label: label != 'SampleID', headers)

    for row in reader_arr:
      meta_data[row['SampleID']] = {}
      for label in labels:
        meta_data[row['SampleID']][label] = row[label]

    return meta_data

def write_results(data, samples, labels, features, results, output_fp, key_map, set_json, feature_ids, args):
  """
  write the indicies listed in results from the features array into fn into a
  tabular output.
  """
  reduced_set = []
  reduced_ids = []
  dels = re.compile('"| |\[|\]')
  for k in range(len(results)):
    mstr = features[int(results[k])]
    reduced_set.append(dels.sub("", mstr))
    reduced_ids.append(feature_ids[int(results[k])])

  ds = get_stats(data, labels, results, reduced_set, key_map)
  ds["Feature IDs"] = reduced_ids

  # JSON output
  if set_json==True:
    json.dump(ds, open(output_fp,"w"))

  # CSV output
  else:
    with open(output_fp, 'w') as f:
      labels = filter(lambda x: x != "Features" and x != "Feature IDs", ds.keys())

      # write header
      f.write("Features\tFeature IDs\t")
      f.write("\t".join(labels)+"\n")
      
      # write each row.
      for n in range(len(ds["Features"])):
        f.write(ds["Features"][n]+"\t"+ds["Feature IDs"][n]+"\t")
        f.write("\t".join([str(ds[x][n]) for x in labels])+"\n")
  
  if args.output_biom is not None:
    write_reduced_biom(data, samples, features, feature_ids, results, args.output_biom)


def get_stats(data, labels, indices, reduced_set, key_map):
  """
  Convert a few statistics about the data to a dictionary. 
  """
  data_struct = {}
  data_rel = count2abun(data)
  for n,key in enumerate(key_map):
    feature_idx = numpy.array([int(i) for i in indices]).reshape( \
        len(indices),1)
    class_idx = numpy.where(numpy.array(labels)==key_map[key])[0]
    data_struct[key+" (mean)"] = data_rel[class_idx, feature_idx].mean(axis=1).tolist()
    data_struct[key+" (std)"] = data_rel[class_idx, feature_idx].std(axis=1).tolist()
  data_struct["Features"] = reduced_set
  return data_struct

def count2abun(count_matrix):
  """
  Convert X into a relative abundance matrix
  """
  scale_factor = count_matrix.sum(axis=1)
  return count_matrix/numpy.tile(scale_factor,[count_matrix.shape[1],1]).transpose()


def get_fs_methods():
  """
  get_fs_methods()
  return the feature selection methods that are 
  available for use in a list. note that the options
  are case sensitive.
  """
  return ["CIFE","CMIM","CondMI","Condred","ICAP","JMI","MIM","MIFS","mRMR"]

def convert_to_discrete(items):
  """Convert a list of labels to numerics"""
  map_dic = {}
  discrete_arr = []

  # skip the "sample"
  disc_val = 0
  for item in items:
    if item not in map_dic:
       map_dic[item] = disc_val
       disc_val += 1
    discrete_arr.append(map_dic[item])
  return (map_dic, discrete_arr)

def write_reduced_biom(data, samples, features, feature_ids, sf, output_fn):
  """
  writes relative abundance of top selected features into new biom file.
  """
  import datetime

  biom = {}
  biom['id'] = None
  biom['format'] = u'Biological Observation Matrix 1.0.0'
  biom['data'] = []
  biom['rows'] = []
  biom['columns'] = []
  biom['matrix_element_type'] = u'float'
  biom['matrix_type'] = u'dense'
  biom['type'] = u'OTU table'
  biom['date'] = str(datetime.datetime.now())

  data = count2abun(data)

  reduced_data = []

  for row in data:
    temp_row = []
    for index in sf:
      index = int(index)
      temp_row.append(row[index])
    reduced_data.append(temp_row)

  for index in range(len(data)):
    cols_dic = {}
    cols_dic[u'id'] = samples[index]
    cols_dic[u'metadata'] = {}
    biom['columns'].append(cols_dic)

  for index in sf:
    index = int(index)
    # feature row
    rows_dic = {} 
    rows_dic[u'id'] = feature_ids[index]
    rows_dic[u'metadata'] = { u'taxonomy': features[index] }
    biom['rows'].append(rows_dic)

  # data
  biom['data'] = reduced_data
  json.dump(biom, open(output_fn, "w"))



def build_parser():
  parser = argparse.ArgumentParser(
    description=("NPFS implements feature subset selection for biological "
      "data formats, which are commonly used in metagenomic data analysis.\n")
  )
  parser.add_argument("-a", "--alpha",
    type=float,
    help="size of the Neyman-Pearson hypothesis test",
    default=0.01)
  parser.add_argument("-b", "--bootstraps",
    type=int,
    help="number of bootstraps to run",
    default=50)
  parser.add_argument("-c", "--cpus",
    type=int,
    help="the number of bootstraps to run simultaneously",
    default=multiprocessing.cpu_count())
  parser.add_argument("-f", "--fs-method",
    help="Feature selection method. Available: "
      +"CIFE CMIM CondMI Condred ICAP JMI MIM MIFS mRMR",
    default="MIM")
  parser.add_argument("-i", "--input-file",
    help="biom format file",
    required=True)
  parser.add_argument("-j", "--json", 
    help="store the output as a json format file.",
    action="store_true",
    required=False)
  parser.add_argument("-l", "--label",
    help="name of column of the mapping file that "
      +"indicates the labels",
    required=True)
  parser.add_argument("-m", "--map-file",
    help="map file (tsv)",
    required=True)
  parser.add_argument("-n", "--select",
    type=int, 
    help="number of features to start with",
    default=5)
  parser.add_argument("-o", "--output-file",
    help="output file where selected OTU IDs"
    +"are stored",
    required=True)
  parser.add_argument("-s", "--scale",
    help="scale the biom table to account for the relative abundance.",
    action="store_true",
    required=False)
  parser.add_argument("-z", "--log-scale",
    help="perform a log-scale the biom table to account for the relative abundance.",
    action="store_true",
    required=False)
  parser.add_argument("-r", "--output-biom",
    help="output a BIOM file with the relative abundances of the sub-matrix of"
    " the selected features.",
    required=False)
  return parser

def main():
  parser = build_parser()
  args = parser.parse_args()

  try:
    global feast
    import feast
  except ImportError:
    parser.error("Error loading the PyFeast module. is PyFeast installed?")

  try:
    global npfs
    import npfs
  except ImportError:
    parser.error("Error loading the NPFS module. is NPFS installed?")

  # Make sure our input exist
  if not os.path.isfile(args.input_file):
    parser.error("Input file not found.")

  if not os.path.isfile(args.map_file):
    parser.error("Map file not found.")

  if args.select < 1:
    parser.error("You must select at least one result.")

  if args.fs_method not in get_fs_methods():
    parser.error("Feature selection method not found. please select from " 
      + " ".join(get_fs_methods()))

  data, samples, features, feature_ids = load_biom(args)
  data = numpy.array(data)

  map_arr = load_map(args.map_file)

  labels = []
  for sample_id in samples:
    if sample_id not in map_arr:
      exit('Error: sample ' + sample_id + ' was not found in map file.')
    if args.label not in map_arr[sample_id]:
      exit('Error: label ' + args.label + ' was not found in map file.')
    labels.append(map_arr[sample_id][args.label])

  labels_disc_dic, labels_disc_arr = convert_to_discrete(labels)

  labels_disc_arr = np.array(labels_disc_arr, order="F")

  mdl = npfs.npfs(fs_method=args.fs_method, n_select=args.select, n_bootstraps=args.bootstraps, 
      verbose=False, alpha=args.alpha, beta=0., parallel=args.cpus)

  results = mdl.fit(data, labels_disc_arr)

  write_results(data, samples, labels_disc_arr, features, results, 
      args.output_file, labels_disc_dic, args.json, feature_ids, args)


if __name__ == "__main__":
  sys.exit(main())
